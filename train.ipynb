{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import simplejson\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import threading\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils import data_utils, train_utils\n",
    "import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_data = train_utils.input_data(class_id=0, \n",
    "#                                    crop_per_img=1, reflection=True, train=True, crop_size=572, rotation=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def argument_scope(H, phase):\n",
    "    '''\n",
    "    This returns the arg_scope for slim.arg_scope(), which defines the options for slim.functions\n",
    "    '''\n",
    "    padding = H['padding']\n",
    "    is_training = {'train': True, 'validate': False, 'test': False}[phase]\n",
    "    pool_kernel = [2, 2]\n",
    "    pool_stride = 2\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"decay\": 0.997,\n",
    "        \"epsilon\": 0.001,\n",
    "    }\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d], \n",
    "                        # slim.relu would raise an error here\n",
    "                        activation_fn=tf.nn.relu, \n",
    "                        padding=padding, \n",
    "                        #normalizer_fn=slim.batch_norm, \n",
    "                        normalizer_fn=None,\n",
    "                        weights_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            with slim.arg_scope([slim.max_pool2d], stride=pool_stride, kernel_size=pool_kernel):\n",
    "                with slim.arg_scope([slim.conv2d_transpose], \n",
    "                                    activation_fn=None, \n",
    "                                    normalizer_fn=None,\n",
    "                                    padding=padding, \n",
    "                                    weights_initializer=tf.contrib.layers.variance_scaling_initializer()) as sc:\n",
    "                    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_pred(x_in, H, phase):\n",
    "    '''\n",
    "    This function builds the prediction model\n",
    "    '''\n",
    "    num_class = H['num_class']\n",
    "    \n",
    "    conv_kernel_1 = [1, 1]\n",
    "    conv_kernel_3 = [3, 3]\n",
    "    pool_kernel = [2, 2]\n",
    "    pool_stride = 2\n",
    "\n",
    "    early_feature = {}\n",
    "    reuse = {'train': False, 'validate': True, 'test': False}[phase]\n",
    "    \n",
    "    with slim.arg_scope(argument_scope(H, phase)):\n",
    "        \n",
    "        scope_name = 'block_1'\n",
    "        x_input = x_in\n",
    "        num_outputs = 64\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "        \n",
    "        scope_name = 'block_2'\n",
    "        x_input = slim.max_pool2d(layer_2)\n",
    "        num_outputs = 128\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "\n",
    "        scope_name = 'block_3'\n",
    "        x_input = slim.max_pool2d(layer_2)\n",
    "        num_outputs = 256\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "            \n",
    "        scope_name = 'block_4'\n",
    "        x_input = slim.max_pool2d(layer_2)\n",
    "        num_outputs = 512\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "\n",
    "        scope_name = 'block_5'\n",
    "        x_input = slim.max_pool2d(layer_2)\n",
    "        num_outputs = 1024\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "            \n",
    "        scope_name = 'block_6'\n",
    "        num_outputs = 512\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            trans_layer = slim.conv2d_transpose(\n",
    "                layer_2, num_outputs, pool_kernel, pool_stride, scope='conv_trans')\n",
    "            x_input = tf.concat([early_feature['block_4'], trans_layer], axis=3)\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "            \n",
    "        scope_name = 'block_7'\n",
    "        num_outputs = 256\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            trans_layer = slim.conv2d_transpose(\n",
    "                layer_2, num_outputs, pool_kernel, pool_stride, scope='conv_trans')\n",
    "            x_input = tf.concat([early_feature['block_3'], trans_layer], axis=3)\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "            \n",
    "        scope_name = 'block_8'\n",
    "        num_outputs = 128\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            trans_layer = slim.conv2d_transpose(\n",
    "                layer_2, num_outputs, pool_kernel, pool_stride, scope='conv_trans')\n",
    "            x_input = tf.concat([early_feature['block_2'], trans_layer], axis=3)\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "        \n",
    "        scope_name = 'block_9'\n",
    "        num_outputs = 64\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            trans_layer = slim.conv2d_transpose(\n",
    "                layer_2, num_outputs, pool_kernel, pool_stride, scope='conv_trans')\n",
    "            x_input = tf.concat([early_feature['block_1'], trans_layer], axis=3)\n",
    "            layer_1 = slim.conv2d(x_input, num_outputs, conv_kernel_3, scope='conv1')\n",
    "            layer_2 = slim.conv2d(layer_1, num_outputs, conv_kernel_3, scope='conv2')\n",
    "            early_feature[scope_name] = layer_2\n",
    "        \n",
    "        scope_name = 'pred'\n",
    "        with tf.variable_scope(scope_name, reuse = reuse):\n",
    "            # layer_1 = slim.conv2d(layer_2, num_class, conv_kernel_1, scope='conv1', activation_fn=None, normalizer_fn=None)\n",
    "            layer_1 = slim.conv2d(layer_2, 1, conv_kernel_1, scope='conv1', activation_fn=None, normalizer_fn=None)\n",
    "\n",
    "            early_feature[scope_name] = layer_1\n",
    "            \n",
    "            # pred = tf.argmax(tf.nn.softmax(logits=layer_1), axis=3)\n",
    "            pred = tf.sigmoid(layer_1)\n",
    "                \n",
    "        return tf.squeeze(layer_1), tf.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(x_in, y_in, H, phase):\n",
    "    '''\n",
    "    This function builds the loss and accuracy\n",
    "    '''\n",
    "    im_width = H['im_width']\n",
    "    im_height = H['im_height']\n",
    "    batch_size = H['batch_size']\n",
    "    start_ind = H['start_ind']\n",
    "    valid_size = H['valid_size']\n",
    "    num_class = H['num_class']\n",
    "    epsilon = H['epsilon']\n",
    "    \n",
    "    logits, pred = build_pred(x_in, H, phase)\n",
    "    y_crop = tf.cast(tf.slice(y_in, begin=[0, start_ind, start_ind], size=[-1, valid_size, valid_size]), tf.float32)\n",
    "    logits_crop = tf.slice(logits, \n",
    "                         begin=[0, start_ind, start_ind], \n",
    "                         size=[-1, valid_size, valid_size])\n",
    "    pred_crop = tf.cast(tf.slice(pred, \n",
    "                             begin=[0, start_ind, start_ind], \n",
    "                             size=[-1, valid_size, valid_size]), tf.float32)\n",
    "    \n",
    "    # formulation of weighted cross entropy loss, dice index: https://arxiv.org/pdf/1707.03237.pdf\n",
    "    if H['loss_function'] == 'cross_entropy':\n",
    "        loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=y_crop, logits=logits_crop, pos_weight=20.))\n",
    "\n",
    "    elif H['loss_function'] == 'dice':\n",
    "        \n",
    "        intersection = tf.reduce_sum(tf.multiply(y_crop, pred_crop))\n",
    "        union = tf.reduce_sum(tf.square(y_crop)) + tf.reduce_sum(tf.square(pred_crop))\n",
    "        loss = 1. - 2 * intersection / (union + tf.constant(epsilon))\n",
    "                \n",
    "    elif H['loss_function'] == 'jaccard':\n",
    "        intersection = tf.reduce_sum(tf.multiply(y_crop, pred_crop))\n",
    "        union = tf.reduce_sum(y_crop) + tf.reduce_sum(pred_crop) - intersection\n",
    "        loss = 1. - intersection / (union + tf.constant(epsilon))\n",
    "        \n",
    "    pred_thres = tf.cast(tf.greater(pred_crop, 0.5), tf.float32)\n",
    "    inter = tf.reduce_sum(tf.multiply(tf.cast(y_crop, tf.float32), pred_thres))\n",
    "    uni = tf.reduce_sum(tf.cast(y_crop, tf.float32)) + tf.reduce_sum(pred_thres) - inter\n",
    "    jaccard = inter / (uni + tf.constant(epsilon))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.greater(pred_crop, 0.5), tf.int32), \n",
    "    #                                           tf.cast(y_crop, tf.int32)), tf.float32))\n",
    "    \n",
    "    return loss, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build(queues, H):\n",
    "    '''\n",
    "    This function returns the train operation, summary, global step\n",
    "    '''\n",
    "    im_width = H['im_width']\n",
    "    im_height = H['im_height']\n",
    "    num_class = H['num_class']\n",
    "    num_channel = H['num_channel']\n",
    "    batch_size = H['batch_size']\n",
    "    log_dir = H['log_dir']\n",
    "    norm_threshold = H['norm_threshold']\n",
    "    \n",
    "    loss, accuracy, x_in, y_in = {}, {}, {}, {}\n",
    "    for phase in ['train', 'validate']:\n",
    "        x_in[phase], y_in[phase] = queues[phase].dequeue_many(batch_size)\n",
    "        loss[phase], accuracy[phase] = build_loss(x_in[phase], y_in[phase], H, phase)\n",
    "    \n",
    "    learning_rate = tf.placeholder(dtype=tf.float32)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads= tf.gradients(loss['train'], tvars)\n",
    "    grads, norm = tf.clip_by_global_norm(grads, norm_threshold)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = opt.apply_gradients(zip(grads, tvars), global_step = global_step)\n",
    "    \n",
    "    for phase in ['train', 'validate']:\n",
    "        tf.summary.scalar(name=phase + '/loss', tensor=loss[phase])\n",
    "        tf.summary.scalar(name=phase + '/loss', tensor=loss[phase])\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return loss, accuracy, train_op, summary_op, learning_rate, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hypes = './hypes/hypes.json'\n",
    "with open(hypes, 'r') as f:\n",
    "    H = simplejson.load(f)\n",
    "    # H['loss_function'] = 'dice'\n",
    "    H['loss_function'] == 'cross_entropy'\n",
    "    im_width = H['im_width']\n",
    "    im_height = H['im_height']\n",
    "    num_class = H['num_class']\n",
    "    num_channel = H['num_channel']\n",
    "    queue_size = H['queue_size']\n",
    "    save_iter = H['save_iter']\n",
    "    print_iter = H['print_iter']\n",
    "    class_type = H['class_type']\n",
    "    train_iter = H['train_iter']\n",
    "    lr = H['lr']\n",
    "    lr_decay_iter = H['lr_decay_iter']\n",
    "    log_dir = H['log_dir']\n",
    "    batch_size = H['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def enqueue_thread(sess, data_gen, coord, phase, enqueue_op):\n",
    "    while not coord.should_stop():\n",
    "        img, label = data_gen.next()\n",
    "        sess.run(enqueue_op, feed_dict={x_in[phase]: img, y_in[phase]: label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_in, y_in, queues, enqueue_op = {}, {}, {}, {}\n",
    "shape = ((im_width, im_height, num_channel), \n",
    "         (im_width, im_height))\n",
    "for phase in ['train', 'validate']:\n",
    "    x_in[phase] = tf.placeholder(dtype=tf.float32)\n",
    "    y_in[phase] = tf.placeholder(dtype=tf.float32)\n",
    "    queues[phase] = tf.FIFOQueue(capacity=queue_size, shapes=shape, dtypes=(tf.float32,tf.float32))\n",
    "    enqueue_op[phase] = queues[phase].enqueue_many([x_in[phase], y_in[phase]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utils/data_utils.py:647: RuntimeWarning: divide by zero encountered in divide\n",
      "  evi = (nir - image_r) / (nir + C1 * image_r - C2 * image_b + L)\n",
      "utils/data_utils.py:647: RuntimeWarning: invalid value encountered in divide\n",
      "  evi = (nir - image_r) / (nir + C1 * image_r - C2 * image_b + L)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: [=] 100%\n",
      "Loading validation data: [=] 100%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, train_op, summary_op, learning_rate, global_step = build(queues, H)\n",
    "data_gen = {}\n",
    "for phase in ['train', 'validate']:\n",
    "    is_train = {'train': True, 'validate': False}[phase]\n",
    "    data_gen[phase] = train_utils.input_data(crop_per_img=1, class_id=class_type, reflection=True,\n",
    "                                             rotation=360, train=is_train, crop_size=im_width)\n",
    "    # Run the generator once to make sure the data is loaded into the memory\n",
    "    # This will take a few minutes\n",
    "    data_gen[phase].next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step (1): LR: 0.01000; Train loss 791.44; Train accuracy 0%; Validate loss 659.51; Validate accuracy 10%; Time / image: 0.0ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (11): LR: 0.01000; Train loss 1.70; Train accuracy 7%; Validate loss 0.71; Validate accuracy 0%; Time / image: 1001.2ms\n",
      "\n",
      "Global step (21): LR: 0.01000; Train loss 1.27; Train accuracy 0%; Validate loss 1.99; Validate accuracy 13%; Time / image: 644.6ms\n",
      "\n",
      "Global step (31): LR: 0.01000; Train loss 1.99; Train accuracy 12%; Validate loss 2.01; Validate accuracy 12%; Time / image: 644.6ms\n",
      "\n",
      "Global step (41): LR: 0.01000; Train loss 1.94; Train accuracy 10%; Validate loss 2.25; Validate accuracy 22%; Time / image: 642.2ms\n",
      "\n",
      "Global step (51): LR: 0.01000; Train loss 1.32; Train accuracy 4%; Validate loss 0.96; Validate accuracy 0%; Time / image: 643.4ms\n",
      "\n",
      "Global step (61): LR: 0.01000; Train loss 1.40; Train accuracy 5%; Validate loss 1.20; Validate accuracy 2%; Time / image: 644.4ms\n",
      "\n",
      "Global step (71): LR: 0.01000; Train loss 2.33; Train accuracy 18%; Validate loss 1.79; Validate accuracy 9%; Time / image: 646.8ms\n",
      "\n",
      "Global step (81): LR: 0.01000; Train loss 1.39; Train accuracy 0%; Validate loss 1.56; Validate accuracy 3%; Time / image: 642.3ms\n",
      "\n",
      "Global step (91): LR: 0.01000; Train loss 1.23; Train accuracy 2%; Validate loss 1.04; Validate accuracy 0%; Time / image: 651.1ms\n",
      "\n",
      "Global step (101): LR: 0.01000; Train loss 1.83; Train accuracy 9%; Validate loss 2.94; Validate accuracy 20%; Time / image: 647.5ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (111): LR: 0.01000; Train loss 0.93; Train accuracy 0%; Validate loss 1.89; Validate accuracy 10%; Time / image: 657.9ms\n",
      "\n",
      "Global step (121): LR: 0.01000; Train loss 1.69; Train accuracy 8%; Validate loss 2.15; Validate accuracy 13%; Time / image: 645.0ms\n",
      "\n",
      "Global step (131): LR: 0.01000; Train loss 1.56; Train accuracy 6%; Validate loss 1.25; Validate accuracy 2%; Time / image: 644.7ms\n",
      "\n",
      "Global step (141): LR: 0.01000; Train loss 1.32; Train accuracy 4%; Validate loss 2.63; Validate accuracy 19%; Time / image: 649.9ms\n",
      "\n",
      "Global step (151): LR: 0.01000; Train loss 2.38; Train accuracy 15%; Validate loss 1.46; Validate accuracy 5%; Time / image: 644.5ms\n",
      "\n",
      "Global step (161): LR: 0.01000; Train loss 1.71; Train accuracy 8%; Validate loss 0.98; Validate accuracy 0%; Time / image: 648.0ms\n",
      "\n",
      "Global step (171): LR: 0.01000; Train loss 1.20; Train accuracy 0%; Validate loss 1.57; Validate accuracy 6%; Time / image: 643.4ms\n",
      "\n",
      "Global step (181): LR: 0.01000; Train loss 1.50; Train accuracy 4%; Validate loss 2.08; Validate accuracy 15%; Time / image: 644.0ms\n",
      "\n",
      "Global step (191): LR: 0.01000; Train loss 1.59; Train accuracy 5%; Validate loss 1.57; Validate accuracy 5%; Time / image: 647.9ms\n",
      "\n",
      "Global step (481): LR: 0.01000; Train loss 2.44; Train accuracy 19%; Validate loss 2.37; Validate accuracy 18%; Time / image: 641.8ms\n",
      "\n",
      "Global step (491): LR: 0.01000; Train loss 1.58; Train accuracy 6%; Validate loss 1.25; Validate accuracy 1%; Time / image: 643.6ms\n",
      "\n",
      "Global step (501): LR: 0.01000; Train loss 2.27; Train accuracy 17%; Validate loss 1.47; Validate accuracy 5%; Time / image: 643.6ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (511): LR: 0.01000; Train loss 1.24; Train accuracy 1%; Validate loss 2.24; Validate accuracy 16%; Time / image: 658.6ms\n",
      "\n",
      "Global step (521): LR: 0.01000; Train loss 1.95; Train accuracy 12%; Validate loss 1.42; Validate accuracy 4%; Time / image: 644.7ms\n",
      "\n",
      "Global step (531): LR: 0.01000; Train loss 1.94; Train accuracy 11%; Validate loss 1.57; Validate accuracy 6%; Time / image: 646.7ms\n",
      "\n",
      "Global step (541): LR: 0.01000; Train loss 1.60; Train accuracy 7%; Validate loss 2.34; Validate accuracy 17%; Time / image: 646.4ms\n",
      "\n",
      "Global step (551): LR: 0.01000; Train loss 1.44; Train accuracy 4%; Validate loss 1.49; Validate accuracy 5%; Time / image: 644.1ms\n",
      "\n",
      "Global step (561): LR: 0.01000; Train loss 1.10; Train accuracy 0%; Validate loss 1.32; Validate accuracy 3%; Time / image: 643.7ms\n",
      "\n",
      "Global step (571): LR: 0.01000; Train loss 2.45; Train accuracy 19%; Validate loss 1.77; Validate accuracy 9%; Time / image: 646.3ms\n",
      "\n",
      "Global step (581): LR: 0.01000; Train loss 2.60; Train accuracy 21%; Validate loss 1.91; Validate accuracy 11%; Time / image: 643.9ms\n",
      "\n",
      "Global step (591): LR: 0.01000; Train loss 1.19; Train accuracy 0%; Validate loss 2.05; Validate accuracy 14%; Time / image: 645.9ms\n",
      "\n",
      "Global step (601): LR: 0.01000; Train loss 1.77; Train accuracy 9%; Validate loss 1.94; Validate accuracy 12%; Time / image: 647.4ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (611): LR: 0.01000; Train loss 2.13; Train accuracy 15%; Validate loss 2.05; Validate accuracy 14%; Time / image: 661.8ms\n",
      "\n",
      "Global step (621): LR: 0.01000; Train loss 1.81; Train accuracy 10%; Validate loss 2.26; Validate accuracy 17%; Time / image: 645.0ms\n",
      "\n",
      "Global step (631): LR: 0.01000; Train loss 1.69; Train accuracy 8%; Validate loss 1.39; Validate accuracy 2%; Time / image: 647.1ms\n",
      "\n",
      "Global step (641): LR: 0.01000; Train loss 1.92; Train accuracy 12%; Validate loss 2.17; Validate accuracy 17%; Time / image: 645.2ms\n",
      "\n",
      "Global step (651): LR: 0.01000; Train loss 1.93; Train accuracy 12%; Validate loss 1.78; Validate accuracy 9%; Time / image: 644.5ms\n",
      "\n",
      "Global step (2571): LR: 0.00500; Train loss 1.95; Train accuracy 12%; Validate loss 1.20; Validate accuracy 0%; Time / image: 641.0ms\n",
      "\n",
      "Global step (2581): LR: 0.00500; Train loss 1.39; Train accuracy 3%; Validate loss 1.21; Validate accuracy 0%; Time / image: 641.4ms\n",
      "\n",
      "Global step (2591): LR: 0.00500; Train loss 1.41; Train accuracy 3%; Validate loss 1.29; Validate accuracy 1%; Time / image: 640.8ms\n",
      "\n",
      "Global step (2601): LR: 0.00500; Train loss 1.33; Train accuracy 2%; Validate loss 1.92; Validate accuracy 12%; Time / image: 641.1ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (2611): LR: 0.00500; Train loss 2.17; Train accuracy 16%; Validate loss 2.34; Validate accuracy 19%; Time / image: 657.3ms\n",
      "\n",
      "Global step (2621): LR: 0.00500; Train loss 1.42; Train accuracy 3%; Validate loss 2.09; Validate accuracy 14%; Time / image: 639.3ms\n",
      "\n",
      "Global step (2631): LR: 0.00500; Train loss 1.23; Train accuracy 0%; Validate loss 2.19; Validate accuracy 16%; Time / image: 638.6ms\n",
      "\n",
      "Global step (2641): LR: 0.00500; Train loss 2.30; Train accuracy 18%; Validate loss 1.66; Validate accuracy 7%; Time / image: 641.5ms\n",
      "\n",
      "Global step (2651): LR: 0.00500; Train loss 1.77; Train accuracy 9%; Validate loss 2.71; Validate accuracy 25%; Time / image: 641.1ms\n",
      "\n",
      "Global step (2661): LR: 0.00500; Train loss 1.83; Train accuracy 10%; Validate loss 1.38; Validate accuracy 2%; Time / image: 641.8ms\n",
      "\n",
      "Global step (2671): LR: 0.00500; Train loss 2.77; Train accuracy 26%; Validate loss 1.86; Validate accuracy 11%; Time / image: 642.0ms\n",
      "\n",
      "Global step (2681): LR: 0.00500; Train loss 2.03; Train accuracy 14%; Validate loss 2.07; Validate accuracy 14%; Time / image: 640.3ms\n",
      "\n",
      "Global step (2691): LR: 0.00500; Train loss 1.98; Train accuracy 13%; Validate loss 2.50; Validate accuracy 22%; Time / image: 643.2ms\n",
      "\n",
      "Global step (2701): LR: 0.00500; Train loss 1.46; Train accuracy 4%; Validate loss 2.54; Validate accuracy 22%; Time / image: 643.6ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (2711): LR: 0.00500; Train loss 1.22; Train accuracy 0%; Validate loss 1.46; Validate accuracy 4%; Time / image: 659.2ms\n",
      "\n",
      "Global step (2721): LR: 0.00500; Train loss 2.01; Train accuracy 13%; Validate loss 1.70; Validate accuracy 8%; Time / image: 644.5ms\n",
      "\n",
      "Global step (2731): LR: 0.00500; Train loss 1.44; Train accuracy 4%; Validate loss 1.85; Validate accuracy 10%; Time / image: 642.9ms\n",
      "\n",
      "Global step (2741): LR: 0.00500; Train loss 2.87; Train accuracy 27%; Validate loss 2.48; Validate accuracy 21%; Time / image: 642.2ms\n",
      "\n",
      "Global step (2751): LR: 0.00500; Train loss 1.72; Train accuracy 8%; Validate loss 1.68; Validate accuracy 8%; Time / image: 641.9ms\n",
      "\n",
      "Global step (2761): LR: 0.00500; Train loss 1.95; Train accuracy 12%; Validate loss 1.17; Validate accuracy 0%; Time / image: 644.4ms\n",
      "\n",
      "Global step (2771): LR: 0.00500; Train loss 2.06; Train accuracy 14%; Validate loss 1.87; Validate accuracy 11%; Time / image: 645.9ms\n",
      "\n",
      "Global step (2781): LR: 0.00500; Train loss 1.66; Train accuracy 7%; Validate loss 1.28; Validate accuracy 1%; Time / image: 647.8ms\n",
      "\n",
      "Global step (2791): LR: 0.00500; Train loss 2.00; Train accuracy 13%; Validate loss 1.81; Validate accuracy 10%; Time / image: 647.4ms\n",
      "\n",
      "Global step (2801): LR: 0.00500; Train loss 2.34; Train accuracy 19%; Validate loss 1.65; Validate accuracy 7%; Time / image: 645.3ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (2811): LR: 0.00500; Train loss 1.67; Train accuracy 7%; Validate loss 1.69; Validate accuracy 7%; Time / image: 662.0ms\n",
      "\n",
      "Global step (2821): LR: 0.00500; Train loss 1.22; Train accuracy 0%; Validate loss 1.97; Validate accuracy 12%; Time / image: 644.8ms\n",
      "\n",
      "Global step (2831): LR: 0.00500; Train loss 1.19; Train accuracy 0%; Validate loss 1.69; Validate accuracy 8%; Time / image: 646.3ms\n",
      "\n",
      "Global step (2841): LR: 0.00500; Train loss 1.31; Train accuracy 2%; Validate loss 2.50; Validate accuracy 21%; Time / image: 647.9ms\n",
      "\n",
      "Global step (2851): LR: 0.00500; Train loss 1.86; Train accuracy 11%; Validate loss 1.16; Validate accuracy 0%; Time / image: 644.4ms\n",
      "\n",
      "Global step (2861): LR: 0.00500; Train loss 1.16; Train accuracy 0%; Validate loss 2.45; Validate accuracy 20%; Time / image: 644.3ms\n",
      "\n",
      "Global step (2871): LR: 0.00500; Train loss 1.28; Train accuracy 2%; Validate loss 1.18; Validate accuracy 0%; Time / image: 641.0ms\n",
      "\n",
      "Global step (2881): LR: 0.00500; Train loss 1.99; Train accuracy 13%; Validate loss 1.86; Validate accuracy 11%; Time / image: 640.6ms\n",
      "\n",
      "Global step (2891): LR: 0.00500; Train loss 1.80; Train accuracy 9%; Validate loss 1.16; Validate accuracy 0%; Time / image: 643.9ms\n",
      "\n",
      "Global step (2901): LR: 0.00500; Train loss 2.12; Train accuracy 15%; Validate loss 1.31; Validate accuracy 2%; Time / image: 649.5ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (2911): LR: 0.00500; Train loss 1.38; Train accuracy 3%; Validate loss 1.44; Validate accuracy 4%; Time / image: 662.6ms\n",
      "\n",
      "Global step (2921): LR: 0.00500; Train loss 1.47; Train accuracy 4%; Validate loss 1.17; Validate accuracy 0%; Time / image: 648.6ms\n",
      "\n",
      "Global step (2931): LR: 0.00500; Train loss 1.72; Train accuracy 8%; Validate loss 1.53; Validate accuracy 5%; Time / image: 647.8ms\n",
      "\n",
      "Global step (2941): LR: 0.00500; Train loss 1.84; Train accuracy 10%; Validate loss 1.99; Validate accuracy 13%; Time / image: 646.4ms\n",
      "\n",
      "Global step (2951): LR: 0.00500; Train loss 1.21; Train accuracy 0%; Validate loss 1.52; Validate accuracy 5%; Time / image: 644.4ms\n",
      "\n",
      "Global step (2961): LR: 0.00500; Train loss 2.34; Train accuracy 18%; Validate loss 2.17; Validate accuracy 15%; Time / image: 644.1ms\n",
      "\n",
      "Global step (2971): LR: 0.00500; Train loss 2.16; Train accuracy 15%; Validate loss 2.06; Validate accuracy 14%; Time / image: 645.7ms\n",
      "\n",
      "Global step (2981): LR: 0.00500; Train loss 1.38; Train accuracy 3%; Validate loss 1.32; Validate accuracy 2%; Time / image: 645.2ms\n",
      "\n",
      "Global step (2991): LR: 0.00500; Train loss 1.39; Train accuracy 3%; Validate loss 2.18; Validate accuracy 15%; Time / image: 646.1ms\n",
      "\n",
      "Global step (3001): LR: 0.00500; Train loss 2.18; Train accuracy 15%; Validate loss 1.81; Validate accuracy 10%; Time / image: 645.6ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (3011): LR: 0.00500; Train loss 1.28; Train accuracy 2%; Validate loss 1.22; Validate accuracy 1%; Time / image: 664.9ms\n",
      "\n",
      "Global step (3021): LR: 0.00500; Train loss 1.84; Train accuracy 10%; Validate loss 1.18; Validate accuracy 0%; Time / image: 645.2ms\n",
      "\n",
      "Global step (3031): LR: 0.00500; Train loss 1.26; Train accuracy 1%; Validate loss 1.42; Validate accuracy 4%; Time / image: 643.6ms\n",
      "\n",
      "Global step (3041): LR: 0.00500; Train loss 1.49; Train accuracy 4%; Validate loss 1.75; Validate accuracy 9%; Time / image: 645.6ms\n",
      "\n",
      "Global step (3051): LR: 0.00500; Train loss 1.18; Train accuracy 0%; Validate loss 1.18; Validate accuracy 0%; Time / image: 646.1ms\n",
      "\n",
      "Global step (3061): LR: 0.00500; Train loss 1.17; Train accuracy 0%; Validate loss 1.84; Validate accuracy 10%; Time / image: 645.3ms\n",
      "\n",
      "Global step (3071): LR: 0.00500; Train loss 1.93; Train accuracy 12%; Validate loss 2.72; Validate accuracy 24%; Time / image: 644.2ms\n",
      "\n",
      "Global step (3081): LR: 0.00500; Train loss 1.70; Train accuracy 8%; Validate loss 1.79; Validate accuracy 9%; Time / image: 646.0ms\n",
      "\n",
      "Global step (3091): LR: 0.00500; Train loss 1.14; Train accuracy 0%; Validate loss 2.33; Validate accuracy 18%; Time / image: 643.7ms\n",
      "\n",
      "Global step (3101): LR: 0.00500; Train loss 2.02; Train accuracy 13%; Validate loss 2.14; Validate accuracy 15%; Time / image: 646.3ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (3111): LR: 0.00500; Train loss 1.74; Train accuracy 9%; Validate loss 2.21; Validate accuracy 15%; Time / image: 662.2ms\n",
      "\n",
      "Global step (3121): LR: 0.00500; Train loss 1.29; Train accuracy 2%; Validate loss 1.46; Validate accuracy 5%; Time / image: 646.6ms\n",
      "\n",
      "Global step (3131): LR: 0.00500; Train loss 2.23; Train accuracy 16%; Validate loss 1.54; Validate accuracy 6%; Time / image: 644.8ms\n",
      "\n",
      "Global step (3141): LR: 0.00500; Train loss 1.11; Train accuracy 0%; Validate loss 1.77; Validate accuracy 9%; Time / image: 644.5ms\n",
      "\n",
      "Global step (3151): LR: 0.00500; Train loss 1.94; Train accuracy 11%; Validate loss 2.92; Validate accuracy 26%; Time / image: 645.9ms\n",
      "\n",
      "Global step (3161): LR: 0.00500; Train loss 1.19; Train accuracy 1%; Validate loss 2.16; Validate accuracy 15%; Time / image: 644.9ms\n",
      "\n",
      "Global step (3171): LR: 0.00500; Train loss 2.20; Train accuracy 15%; Validate loss 1.11; Validate accuracy 0%; Time / image: 637.3ms\n",
      "\n",
      "Global step (3181): LR: 0.00500; Train loss 1.10; Train accuracy 0%; Validate loss 1.95; Validate accuracy 12%; Time / image: 633.4ms\n",
      "\n",
      "Global step (3191): LR: 0.00500; Train loss 2.27; Train accuracy 16%; Validate loss 1.61; Validate accuracy 7%; Time / image: 635.6ms\n",
      "\n",
      "Global step (3201): LR: 0.00500; Train loss 2.56; Train accuracy 21%; Validate loss 1.62; Validate accuracy 7%; Time / image: 641.0ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (3211): LR: 0.00500; Train loss 2.48; Train accuracy 20%; Validate loss 1.70; Validate accuracy 8%; Time / image: 665.8ms\n",
      "\n",
      "Global step (3221): LR: 0.00500; Train loss 2.18; Train accuracy 15%; Validate loss 2.04; Validate accuracy 13%; Time / image: 652.6ms\n",
      "\n",
      "Global step (3231): LR: 0.00500; Train loss 1.95; Train accuracy 12%; Validate loss 1.48; Validate accuracy 5%; Time / image: 651.7ms\n",
      "\n",
      "Global step (3241): LR: 0.00500; Train loss 2.02; Train accuracy 13%; Validate loss 1.64; Validate accuracy 7%; Time / image: 646.4ms\n",
      "\n",
      "Global step (3251): LR: 0.00500; Train loss 2.40; Train accuracy 19%; Validate loss 1.79; Validate accuracy 9%; Time / image: 634.6ms\n",
      "\n",
      "Global step (3261): LR: 0.00500; Train loss 1.92; Train accuracy 11%; Validate loss 1.20; Validate accuracy 0%; Time / image: 634.5ms\n",
      "\n",
      "Global step (3271): LR: 0.00500; Train loss 2.02; Train accuracy 13%; Validate loss 1.88; Validate accuracy 11%; Time / image: 638.9ms\n",
      "\n",
      "Global step (3281): LR: 0.00500; Train loss 1.60; Train accuracy 6%; Validate loss 1.26; Validate accuracy 0%; Time / image: 645.1ms\n",
      "\n",
      "Global step (3291): LR: 0.00500; Train loss 1.88; Train accuracy 11%; Validate loss 1.24; Validate accuracy 0%; Time / image: 649.6ms\n",
      "\n",
      "Global step (3301): LR: 0.00500; Train loss 1.20; Train accuracy 0%; Validate loss 1.63; Validate accuracy 7%; Time / image: 650.3ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (3311): LR: 0.00500; Train loss 1.43; Train accuracy 3%; Validate loss 1.86; Validate accuracy 10%; Time / image: 661.4ms\n",
      "\n",
      "Global step (3321): LR: 0.00500; Train loss 1.69; Train accuracy 8%; Validate loss 2.01; Validate accuracy 13%; Time / image: 642.9ms\n",
      "\n",
      "Global step (3331): LR: 0.00500; Train loss 1.18; Train accuracy 0%; Validate loss 2.36; Validate accuracy 19%; Time / image: 642.9ms\n",
      "\n",
      "Global step (3341): LR: 0.00500; Train loss 2.03; Train accuracy 13%; Validate loss 1.33; Validate accuracy 2%; Time / image: 642.9ms\n",
      "\n",
      "Global step (3351): LR: 0.00500; Train loss 1.74; Train accuracy 9%; Validate loss 1.32; Validate accuracy 2%; Time / image: 644.0ms\n",
      "\n",
      "Global step (3361): LR: 0.00500; Train loss 2.91; Train accuracy 28%; Validate loss 2.05; Validate accuracy 14%; Time / image: 646.3ms\n",
      "\n",
      "Global step (3371): LR: 0.00500; Train loss 2.02; Train accuracy 13%; Validate loss 1.72; Validate accuracy 8%; Time / image: 644.2ms\n",
      "\n",
      "Global step (3381): LR: 0.00500; Train loss 1.60; Train accuracy 6%; Validate loss 1.19; Validate accuracy 0%; Time / image: 644.1ms\n",
      "\n",
      "Global step (3391): LR: 0.00500; Train loss 1.69; Train accuracy 8%; Validate loss 1.82; Validate accuracy 10%; Time / image: 643.1ms\n",
      "\n",
      "Global step (3401): LR: 0.00500; Train loss 1.48; Train accuracy 4%; Validate loss 1.96; Validate accuracy 12%; Time / image: 643.2ms\n",
      "\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "Global step (3411): LR: 0.00500; Train loss 1.91; Train accuracy 11%; Validate loss 1.46; Validate accuracy 4%; Time / image: 660.6ms\n",
      "\n",
      "Global step (3421): LR: 0.00500; Train loss 1.58; Train accuracy 6%; Validate loss 1.98; Validate accuracy 13%; Time / image: 642.8ms\n",
      "\n",
      "Global step (3431): LR: 0.00500; Train loss 1.37; Train accuracy 2%; Validate loss 1.97; Validate accuracy 13%; Time / image: 644.5ms\n",
      "\n",
      "Global step (3441): LR: 0.00500; Train loss 1.96; Train accuracy 12%; Validate loss 1.92; Validate accuracy 12%; Time / image: 643.7ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "coord = tf.train.Coordinator()\n",
    "threads = {}\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=config).as_default() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(logdir = os.path.join(log_dir, 'summary'), flush_secs = 10)\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    for phase in ['train', 'validate']:\n",
    "        \n",
    "        threads[phase] = threading.Thread(\n",
    "            target=enqueue_thread, \n",
    "            args=(sess, data_gen[phase], coord, phase, enqueue_op[phase]))\n",
    "        threads[phase].start()\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start = time.time()\n",
    "    for step in xrange(train_iter):\n",
    "        if step and step % lr_decay_iter == 0:\n",
    "            lr *= 0.5\n",
    "        \n",
    "        if step % print_iter == 0:\n",
    "            dt = (time.time() - start) / batch_size / print_iter\n",
    "            start = time.time()\n",
    "            _, train_loss, train_accuracy, validate_loss, validate_accuracy, summaries = \\\n",
    "            sess.run([train_op, loss['train'], accuracy['train'], loss['validate'], accuracy['validate'], summary_op],\n",
    "                    feed_dict = {learning_rate: lr})\n",
    "            summary_writer.add_summary(summaries, global_step = global_step.eval())\n",
    "            str0 = 'Global step ({0}): LR: {1:0.5f}; '.format(global_step.eval(), lr)\n",
    "            str1 = 'Train loss {0:.2f}; '.format(train_loss)\n",
    "            str2 = 'Train accuracy {}%; '.format(int(100 * train_accuracy))\n",
    "            str3 = 'Validate loss {0:.2f}; '.format(validate_loss)\n",
    "            str4 = 'Validate accuracy {}%; '.format(int(100 * validate_accuracy))\n",
    "            str5 = 'Time / image: {0:0.1f}ms'.format(1000 * dt if step else 0)\n",
    "            print str0 + str1 + str2 + str3 + str4 + str5 + '\\n'\n",
    "        else:\n",
    "            sess.run([train_op, loss['train']], feed_dict = {learning_rate: lr})\n",
    "        \n",
    "        if step % save_iter == 0:\n",
    "            now = datetime.datetime.now()\n",
    "            path = os.path.join(log_dir, 'ckpt','_'.join(map(str, [now.month, now.day, now.hour, now.minute])))\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            saver.save(sess, path, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# coord.requst_stop()\n",
    "# coord.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Cross validation can be performed at angles different from the training images.\n",
    "\n",
    "Loss options:\n",
    "    1. Jaccard loss\n",
    "    2. Cross entropy, adjust for class imbalance\n",
    "\n",
    "Optimizer options:\n",
    "    1. Adam (learning rate drop at around 0.2 of the initial rate for every \n",
    "    30 epochs)\n",
    "    2. NAdam (no improvement over Adam) (50 epochs with a learning rate \n",
    "    of 1e-3 and additional 50 epochs with a learning rate of 1e-4. Each epoch \n",
    "    was trained on 400 batches, each batch containing 128 image patches (112x112).)\n",
    "\n",
    "Ensembling:\n",
    "    1. Arithmetic averaging over different angles\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in tf.trainable_variables(): print x.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
